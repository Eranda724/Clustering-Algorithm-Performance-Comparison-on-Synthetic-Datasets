{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: hdbscan in c:\\users\\erand\\appdata\\roaming\\python\\python313\\site-packages (0.8.40)\n",
      "Requirement already satisfied: numpy<3,>=1.20 in c:\\users\\erand\\appdata\\roaming\\python\\python313\\site-packages (from hdbscan) (2.2.3)\n",
      "Requirement already satisfied: scipy>=1.0 in c:\\users\\erand\\appdata\\roaming\\python\\python313\\site-packages (from hdbscan) (1.15.2)\n",
      "Requirement already satisfied: scikit-learn>=0.20 in c:\\users\\erand\\appdata\\roaming\\python\\python313\\site-packages (from hdbscan) (1.6.1)\n",
      "Requirement already satisfied: joblib>=1.0 in c:\\users\\erand\\appdata\\roaming\\python\\python313\\site-packages (from hdbscan) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\erand\\appdata\\roaming\\python\\python313\\site-packages (from scikit-learn>=0.20->hdbscan) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install hdbscan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'hdbscan'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcluster\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (KMeans, MiniBatchKMeans, DBSCAN, AgglomerativeClustering,\n\u001b[0;32m      6\u001b[0m                              MeanShift, SpectralClustering, AffinityPropagation, Birch, OPTICS)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmixture\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GaussianMixture\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mhdbscan\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m silhouette_score, calinski_harabasz_score, davies_bouldin_score\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'hdbscan'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons, make_circles\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import (KMeans, MiniBatchKMeans, DBSCAN, AgglomerativeClustering,\n",
    "                             MeanShift, SpectralClustering, AffinityPropagation, Birch, OPTICS)\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import hdbscan\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_datasets(n_samples=1500, random_state=42):\n",
    "    X_moons, y_moons = make_moons(n_samples=n_samples//2, noise=0.05, random_state=random_state)\n",
    "    X_circles, y_circles = make_circles(n_samples=n_samples//2, factor=0.5, noise=0.05, random_state=random_state)\n",
    "    X_combined = np.vstack([X_moons, X_circles])\n",
    "    y_combined = np.hstack([y_moons, y_circles + 2])\n",
    "    return {\n",
    "        'moons': (X_moons, y_moons),\n",
    "        'circles': (X_circles, y_circles),\n",
    "        'combined_moons_circles': (X_combined, y_combined)\n",
    "    }\n",
    "\n",
    "datasets = generate_datasets()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "for ax, (name, (X, y)) in zip(axes, datasets.items()):\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', s=40)\n",
    "    ax.set_title(name.capitalize())\n",
    "    ax.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clustering_algorithms():\n",
    "    return {\n",
    "        'KMeans': KMeans(n_clusters=3, random_state=42),\n",
    "        'MiniBatchKMeans': MiniBatchKMeans(n_clusters=3, random_state=42),\n",
    "        'AffinityPropagation': AffinityPropagation(random_state=42),\n",
    "        'MeanShift': MeanShift(bandwidth=2),\n",
    "        'SpectralClustering': SpectralClustering(n_clusters=3, random_state=42),\n",
    "        'Ward': AgglomerativeClustering(n_clusters=3, linkage='ward'),\n",
    "        'AgglomerativeClustering': AgglomerativeClustering(n_clusters=3),\n",
    "        'DBSCAN': DBSCAN(eps=0.3, min_samples=5),\n",
    "        'HDBSCAN': hdbscan.HDBSCAN(min_cluster_size=15),\n",
    "        'OPTICS': OPTICS(min_samples=5, xi=0.05, min_cluster_size=0.1),\n",
    "        'Birch': Birch(n_clusters=3),\n",
    "        'GaussianMixture': GaussianMixture(n_components=3, random_state=42)\n",
    "    }\n",
    "\n",
    "algorithms = get_clustering_algorithms()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_clustering(X, labels):\n",
    "    if len(np.unique(labels)) < 2:\n",
    "        return None, None, None\n",
    "    silhouette = silhouette_score(X, labels)\n",
    "    calinski = calinski_harabasz_score(X, labels)\n",
    "    davies = davies_bouldin_score(X, labels)\n",
    "    return silhouette, calinski, davies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = []\n",
    "\n",
    "for dataset_name, (X, y_true) in datasets.items():\n",
    "    print(f\"\\nProcessing: {dataset_name}\")\n",
    "    X_scaled = StandardScaler().fit_transform(X)\n",
    "    \n",
    "    for algo_name, algorithm in algorithms.items():\n",
    "        print(f\"  -> {algo_name}\")\n",
    "        start = time.time()\n",
    "        try:\n",
    "            if algo_name == 'MeanShift':\n",
    "                algorithm.fit(X_scaled)\n",
    "                labels = algorithm.labels_\n",
    "            else:\n",
    "                labels = algorithm.fit_predict(X_scaled)\n",
    "            end = time.time()\n",
    "\n",
    "            silhouette, calinski, davies = evaluate_clustering(X_scaled, labels)\n",
    "\n",
    "            all_results.append({\n",
    "                'Dataset': dataset_name,\n",
    "                'Algorithm': algo_name,\n",
    "                'Time (s)': end - start,\n",
    "                'Clusters': len(np.unique(labels)),\n",
    "                'Silhouette': silhouette,\n",
    "                'Calinski': calinski,\n",
    "                'Davies': davies\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error with {algo_name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(all_results)\n",
    "results_df.fillna(\"N/A\", inplace=True)\n",
    "results_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_name in datasets.keys():\n",
    "    subset = results_df[results_df['Dataset'] == dataset_name]\n",
    "    fig, axs = plt.subplots(3, 1, figsize=(14, 12))\n",
    "\n",
    "    sns.barplot(data=subset, x='Algorithm', y='Silhouette', ax=axs[0], palette='Blues_d')\n",
    "    axs[0].set_title(f'Silhouette Score - {dataset_name}')\n",
    "    axs[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "    sns.barplot(data=subset, x='Algorithm', y='Calinski', ax=axs[1], palette='Greens_d')\n",
    "    axs[1].set_title(f'Calinski-Harabasz Score - {dataset_name}')\n",
    "    axs[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "    sns.barplot(data=subset, x='Algorithm', y='Davies', ax=axs[2], palette='Reds_d')\n",
    "    axs[2].set_title(f'Davies-Bouldin Score - {dataset_name} (lower is better)')\n",
    "    axs[2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized = results_df.copy()\n",
    "for dataset in normalized['Dataset'].unique():\n",
    "    mask = normalized['Dataset'] == dataset\n",
    "    normalized.loc[mask, 'Silhouette'] /= normalized.loc[mask, 'Silhouette'].max()\n",
    "    normalized.loc[mask, 'Calinski'] /= normalized.loc[mask, 'Calinski'].max()\n",
    "    normalized.loc[mask, 'Davies'] = normalized.loc[mask, 'Davies'].min() / normalized.loc[mask, 'Davies']\n",
    "    \n",
    "normalized['Combined_Score'] = (normalized['Silhouette'] + normalized['Calinski'] + normalized['Davies']) / 3\n",
    "\n",
    "plt.figure(figsize=(16, 6))\n",
    "sns.barplot(data=normalized, x='Dataset', y='Combined_Score', hue='Algorithm', palette='viridis')\n",
    "plt.title(\"Best Algorithm per Dataset (Combined Score)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display best algorithms\n",
    "for dataset in normalized['Dataset'].unique():\n",
    "    best = normalized[normalized['Dataset'] == dataset].nlargest(1, 'Combined_Score')\n",
    "    print(f\"{dataset}: {best['Algorithm'].values[0]} (Score: {best['Combined_Score'].values[0]:.3f})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
